from bs4 import BeautifulSoup
from datetime import datetime
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, RedirectResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from Src.API.filmpertutti import filmpertutti
from Src.API.streamingcommunity import streaming_community
from Src.API.tantifilm import tantifilm
from Src.API.lordchannel import lordchannel
from Src.API.streamingwatch import streamingwatch
from Src.API.ddlstream import ddlstream
from Src.API.cb01 import cb01
from Src.API.guardaserie import guardaserie
from Src.API.guardahd import guardahd
import Src.Utilities.config as config
import logging
from Src.API.okru import okru_get_url
from Src.API.animeworld import animeworld

# IMPORT ANIME SCRAPERS CON FALLBACK
try:
    from scrapers.animesaturn import AnimeSaturnScraper
    from scrapers.animeunity import AnimeUnityScraper
    from scrapers.gogoanime import GogoAnimeScraper
    ANIME_SCRAPERS_AVAILABLE = True
    print("‚úÖ Anime scrapers loaded successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Anime scrapers not available: {e}")
    ANIME_SCRAPERS_AVAILABLE = False

from Src.Utilities.dictionaries import okru, STREAM, extra_sources, webru_vary, webru_dlhd, provider_map, skystreaming
from Src.API.epg import tivu, tivu_get, epg_guide, convert_bho_1, convert_bho_2, convert_bho_3
from Src.API.webru import webru, get_skystreaming
from Src.API.onlineserietv import onlineserietv
from curl_cffi.requests import AsyncSession
from slowapi import Limiter
from slowapi.util import get_remote_address
from slowapi.middleware import SlowAPIMiddleware
from static.static import HTML
from urllib.parse import unquote
from Src.Utilities.m3u8 import router as m3u8_clone
import urllib.parse
import re
import asyncio

# Configure Env Vars
Global_Proxy = config.Global_Proxy
if Global_Proxy == "1":
    from Src.Utilities.loadenv import load_env
    env_vars = load_env()
    PROXY_CREDENTIALS = env_vars.get('PROXY_CREDENTIALS')
    proxies = {
        "http": PROXY_CREDENTIALS,
        "https": PROXY_CREDENTIALS
    }
else:
    proxies = {}

# Configure config
MYSTERIUS = config.MYSTERIUS
DLHD = config.DLHD
SC = config.SC
SC_DOMAIN = config.SC_DOMAIN
FT = config.FT
TF = config.TF
LC = config.LC
SW = config.SW
AW = config.AW
SKY = config.SKY
CB = config.CB
DDL = config.DDL
GS = config.GS
GHD = config.GHD
OST = config.OST
HOST = config.HOST
PORT = int(config.PORT)
Icon = config.Icon
Name = config.Name
SKY_DOMAIN = config.SKY_DOMAIN
Remote_Instance = config.Remote_Instance

# CONFIGURAZIONI ANIME
AS = getattr(config, 'AS', '0')
AU = getattr(config, 'AU', '0')
GA = getattr(config, 'GA', '0')

# AGGIORNA PROVIDER MAP PER ANIME
if ANIME_SCRAPERS_AVAILABLE:
    provider_map.update({
        'AS': 'ANIMESATURN',
        'AU': 'ANIMEUNITY',
        'GA': 'GOGOANIME'
    })

# INIZIALIZZA ANIME SCRAPERS
anime_scrapers = {}
if ANIME_SCRAPERS_AVAILABLE:
    try:
        if AS == "1":
            anime_scrapers['animesaturn'] = AnimeSaturnScraper()
            print("‚úÖ AnimeSaturn initialized")
        if AU == "1":
            anime_scrapers['animeunity'] = AnimeUnityScraper()
            print("‚úÖ AnimeUnity initialized")
        if GA == "1":
            anime_scrapers['gogoanime'] = GogoAnimeScraper()
            print("‚úÖ GogoAnime initialized")
        print(f"üéå Total anime scrapers: {len(anime_scrapers)}")
    except Exception as e:
        print(f"‚ùå Error initializing anime scrapers: {e}")
        anime_scrapers = {}

if MYSTERIUS == "1":
    from Src.API.cool import cool

DDL_DOMAIN = config.DDL_DOMAIN

# INIZIALIZZA FASTAPI
app = FastAPI()

# CORS MIDDLEWARE OBBLIGATORIO
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(m3u8_clone)
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_middleware(SlowAPIMiddleware)
User_Agent = "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:127.0) Gecko/20100101 Firefox/127.0"

# MANIFEST COMPLETO CON SUPPORTO ANIME
MANIFEST = {
    "id": "org.stremio.mammamia",
    "version": "1.6.0",
    "name": Name,
    "description": "Addon providing HTTPS Streams for Italian Movies, Series, Live TV and Anime from multiple sources!",
    "logo": "https://creazilla-store.fra1.digitaloceanspaces.com/emojis/49647/pizza-emoji-clipart-md.png",
    "resources": ["stream", "catalog", "meta"],
    "types": ["movie", "series", "tv"],
    "idPrefixes": ["tt", "kitsu", "anime"],
    "catalogs": [
        {
            "type": "tv",
            "id": "tv_channels",
            "name": "MammaMia",
            "behaviorHints": {
                "configurable": True,
                "configurationRequired": True
            },
            "extra": [
                {
                    "name": "genre",
                    "isRequired": False,
                    "options": ["Rai", "Mediaset", "Sky", "Euronews", "La7", "Warner Bros", "FIT", "Sportitalia", "RSI", "DAZN", "Rakuten", "Pluto", "A+E", "Paramount", "Chill"]
                }
            ]
        }
    ]
}

# AGGIUNGI CATALOGHI ANIME SE DISPONIBILI
if anime_scrapers:
    MANIFEST["catalogs"].extend([
        {
            "type": "series",
            "id": "anime_search",
            "name": "Cerca Anime",
            "extra": [
                {
                    "name": "search",
                    "isRequired": True
                }
            ]
        },
        {
            "type": "series",
            "id": "anime_trending",
            "name": "Anime Popolari"
        }
    ])

def respond_with(data):
    resp = JSONResponse(data)
    resp.headers['Access-Control-Allow-Origin'] = '*'
    resp.headers['Access-Control-Allow-Headers'] = '*'
    return resp

async def transform_mfp(mfp_stream_url, client):
    try:
        response = await client.get(mfp_stream_url)
        data = response.json()
        url = data['mediaflow_proxy_url'] + "?api_password=" + data['query_params']['api_password'] + "&d=" + urllib.parse.quote(data['destination_url'])
        for i in data['request_headers']:
            url += f"&h_{i}={urllib.parse.quote(data['request_headers'][i])}"
        return url
    except Exception as e:
        print("Transforming MFP failed", e)
        return None

# FUNZIONI ANIME
async def search_anime_multi_source(query: str):
    """Cerca anime su tutti i siti configurati"""
    if not anime_scrapers:
        return []
        
    all_results = []
    
    async def search_site(site_name, scraper):
        try:
            print(f"üîç Searching {site_name} for: {query}")
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(None, scraper.search, query)
            for result in results:
                result['source_site'] = site_name
            return results
        except Exception as e:
            print(f"‚ùå Error searching {site_name}: {e}")
            return []
    
    tasks = [search_site(site_name, scraper) for site_name, scraper in anime_scrapers.items()]
    if tasks:
        results_lists = await asyncio.gather(*tasks, return_exceptions=True)
        for results in results_lists:
            if isinstance(results, list):
                all_results.extend(results)
    
    return all_results

def deduplicate_anime_results(results):
    """Rimuove duplicati basandosi sul titolo normalizzato"""
    seen = {}
    unique_results = []
    
    for result in results:
        normalized_title = re.sub(r'[^\w\s]', '', result['title'].lower()).strip()
        normalized_title = re.sub(r'\s+', ' ', normalized_title)
        
        if normalized_title not in seen:
            seen[normalized_title] = result
            unique_results.append(result)
    
    return unique_results

# ENDPOINT MANIFEST BASE
@app.get('/manifest.json')
def base_manifest():
    print("üìã Base manifest request")
    return respond_with(MANIFEST)

@app.get('/config')
def config_redirect():
    return RedirectResponse(url="/")

@app.get('/{config:path}/manifest.json')
def addon_manifest(config: str):
    print(f"üìã Manifest with config: {config}")
    manifest_copy = MANIFEST.copy()
    
    if "LIVETV" in config:
        return respond_with(manifest_copy)
    elif "LIVETV" not in config:
        # Mantieni anime se disponibili, rimuovi solo TV
        if not anime_scrapers:
            manifest_copy["catalogs"] = []
            if "catalog" in manifest_copy["resources"]:
                manifest_copy["resources"].remove("catalog")
        else:
            manifest_copy["catalogs"] = [cat for cat in manifest_copy["catalogs"] if cat["type"] != "tv"]
        return respond_with(manifest_copy)

@app.get('/', response_class=HTMLResponse)
def root(request: Request):
    forwarded_proto = request.headers.get("x-forwarded-proto")
    scheme = forwarded_proto if forwarded_proto else request.url.scheme
    instance_url = f"{scheme}://{request.url.netloc}"
    html_content = HTML.replace("{instance_url}", instance_url)
    return html_content
@app.get('/test-scrapers')
async def test_scrapers():
    """Endpoint per testare i scrapers anime"""
    print("üß™ Testing scrapers endpoint called")
    
    results = {
        "timestamp": str(datetime.now()),
        "scrapers_available": len(anime_scrapers),
        "scrapers": {}
    }
    
    if not anime_scrapers:
        results["error"] = "No anime scrapers available"
        results["anime_scrapers_loaded"] = ANIME_SCRAPERS_AVAILABLE
        return results
    
    for site_name, scraper in anime_scrapers.items():
        print(f"üîç Testing {site_name}...")
        
        try:
            # Test ricerca base
            search_results = scraper.search("naruto")
            
            site_results = {
                "status": "working",
                "base_url": scraper.base_url,
                "search_results_count": len(search_results),
                "error": None
            }
            
            if search_results:
                first_anime = search_results[0]
                site_results["sample_result"] = {
                    "title": first_anime.get('title', 'No title'),
                    "url": first_anime.get('url', 'No URL'),
                    "image": first_anime.get('image', 'No image')
                }
                
                # Test episodi (solo per il primo risultato)
                try:
                    episodes = scraper.get_episodes(first_anime['url'])
                    site_results["episodes_count"] = len(episodes)
                    
                    if episodes:
                        site_results["sample_episode"] = {
                            "title": episodes[0].get('title', 'No title'),
                            "number": episodes[0].get('number', 'No number'),
                            "url": episodes[0].get('url', 'No URL')
                        }
                        
                        # Test stream (solo per primo episodio)
                        try:
                            streams = scraper.get_stream_links(episodes[0]['url'])
                            site_results["streams_count"] = len(streams)
                            
                            if streams:
                                site_results["sample_stream"] = {
                                    "url": streams[0].get('url', 'No URL'),
                                    "quality": streams[0].get('quality', 'Unknown'),
                                    "type": streams[0].get('type', 'Unknown')
                                }
                        except Exception as stream_error:
                            site_results["stream_error"] = str(stream_error)
                            
                except Exception as episode_error:
                    site_results["episode_error"] = str(episode_error)
            else:
                site_results["status"] = "no_results"
            
            results["scrapers"][site_name] = site_results
            
        except Exception as e:
            results["scrapers"][site_name] = {
                "status": "error",
                "base_url": getattr(scraper, 'base_url', 'Unknown'),
                "error": str(e)
            }
            print(f"‚ùå {site_name} error: {e}")
    
    return results

@app.get('/debug-animesaturn')
async def debug_animesaturn():
    """Debug specifico per AnimeSaturn"""
    if 'animesaturn' not in anime_scrapers:
        return {"error": "AnimeSaturn scraper not available"}
    
    scraper = anime_scrapers['animesaturn']
    
    try:
        # Test connessione base
        response = scraper.make_request(scraper.base_url)
        
        # Test ricerca
        search_url = f"{scraper.base_url}/animelist"
        search_response = scraper.make_request(search_url, params={'search': 'naruto'})
        
        soup = BeautifulSoup(search_response.text, 'html.parser')
        
        return {
            "base_url": scraper.base_url,
            "base_status": response.status_code,
            "search_url": search_url,
            "search_status": search_response.status_code,
            "page_title": soup.title.text if soup.title else "No title",
            "total_links": len(soup.find_all('a')),
            "anime_links": len(soup.find_all('a', href=re.compile(r'/anime/'))),
            "page_content_sample": search_response.text[:500]
        }
        
    except Exception as e:
        return {"error": str(e)}

@app.get('/debug-animesaturn-episode/{episode_id}')
async def debug_episode(episode_id: str):
    """Debug specifico per episodio AnimeSaturn"""
    if 'animesaturn' not in anime_scrapers:
        return {"error": "AnimeSaturn not available"}
    
    scraper = anime_scrapers['animesaturn']
    episode_url = f"https://www.animesaturn.cx/ep/{episode_id}"
    
    try:
        response = scraper.make_request(episode_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        return {
            "episode_url": episode_url,
            "status_code": response.status_code,
            "page_title": soup.title.text if soup.title else "No title",
            "iframes_count": len(soup.find_all('iframe')),
            "iframes": [iframe.get('src') for iframe in soup.find_all('iframe')],
            "has_player_div": bool(soup.find('div', id='player')),
            "page_contains_stream": 'stream' in response.text.lower(),
            "page_size": len(response.text)
        }
        
    except Exception as e:
        return {"error": str(e)}

@app.get('/test-method-exists')
def test_method():
    """Verifica se il metodo esiste"""
    if 'animesaturn' not in anime_scrapers:
        return {"error": "AnimeSaturn not available"}
    
    scraper = anime_scrapers['animesaturn']
    has_method = hasattr(scraper, 'get_stream_links')
    
    return {
        "has_get_stream_links": has_method,
        "scraper_methods": [method for method in dir(scraper) if not method.startswith('_')],
        "scraper_class": scraper.__class__.__name__
    }


# CATALOGHI
async def addon_catalog(type: str, id: str, genre: str = None, search: str = None):
    if type == "tv":
        catalogs = {"metas": []}
        
        for channel in STREAM["channels"]:
            if genre and genre not in channel.get("genres", []):
                continue
            
            description = f'Watch {channel["title"]}'
            catalogs["metas"].append({
                "id": channel["id"],
                "type": "tv",
                "name": channel["title"],
                "poster": channel["poster"],
                "description": description,
                "genres": channel.get("genres", [])
            })

        return catalogs
    
    # CATALOGHI ANIME
    elif type == "series" and id.startswith("anime_") and anime_scrapers:
        catalogs = {"metas": []}
        
        if id == "anime_search" and search:
            print(f"üéå Anime search request: {search}")
            anime_results = await search_anime_multi_source(search)
            anime_results = deduplicate_anime_results(anime_results)
            
            for anime in anime_results[:20]:
                anime_id = f"anime_{anime['source_site']}_{anime['title'].replace(' ', '_').lower()}"
                anime_id = re.sub(r'[^\w_]', '', anime_id)
                
                catalogs["metas"].append({
                    "id": anime_id,
                    "type": "series",
                    "name": anime['title'],
                    "poster": anime.get('image'),
                    "description": f"Anime da {anime['source_site'].upper()}",
                    "genres": ["Anime"]
                })
        
        elif id == "anime_trending":
            # Placeholder per anime popolari
            catalogs["metas"] = []
        
        return catalogs
    else:
        raise HTTPException(status_code=404)

@app.get('/{config:path}/catalog/{type}/{id}.json')
@limiter.limit("5/second")
async def first_catalog(request: Request, type: str, id: str, genre: str = None):
    search = request.query_params.get('search')
    catalogs = await addon_catalog(type, id, genre, search)
    return respond_with(catalogs)

@app.get('/{config:path}/catalog/{type}/{id}/genre={genre}.json')
async def catalog_with_genre(type: str, id: str, genre: str = None):
    catalogs = await addon_catalog(type, id, genre)
    return respond_with(catalogs)

@app.get('/{config:path}/catalog/{type}/{id}/search={search}.json')
async def catalog_with_search(request: Request, type: str, id: str, search: str):
    catalogs = await addon_catalog(type, id, search=search)
    return respond_with(catalogs)

# META ENDPOINTS
@app.get('/{config:path}/meta/tv/{id}.json')
@limiter.limit("20/second")
async def addon_meta(request: Request, id: str):
    channel = next((ch for ch in STREAM['channels'] if ch['id'] == id), None)
    
    if not channel:
        raise HTTPException(status_code=404, detail="Channel not found")
    
    async with AsyncSession(proxies=proxies) as client:
        if channel["id"] in convert_bho_1 or channel["id"] in convert_bho_2 or channel["id"] in convert_bho_3:
            description, title = await epg_guide(channel["id"], client)
        elif channel["id"] in tivu:
            description = await tivu_get(channel["id"], client)
            title = ""
        else:
            description = f'Watch {channel["title"]}'
            title = ""
    
    meta = {
        'meta': {
            'id': channel['id'],
            'type': 'tv',
            'name': channel['name'],
            'poster': channel['poster'],
            'posterShape': 'landscape',
            'description': title + "\n" + description,
            'background': channel['poster'],
            'logo': channel['poster'],
            'genres': channel.get('genres', []),
        }
    }
    if 'url' in channel:
        meta['meta']['url'] = channel['url']
    return respond_with(meta)

@app.get('/{config:path}/meta/movie/{id}.json')
@limiter.limit("20/second")
async def movie_meta(request: Request, id: str):
    print(f"üé¨ Movie meta request: {id}")
    
    meta = {
        'meta': {
            'id': id,
            'type': 'movie',
            'name': f'Film {id}',
            'poster': None,
            'description': f"Film disponibile tramite provider configurati",
            'genres': ['Movie']
        }
    }
    return respond_with(meta)

@app.get('/{config:path}/meta/series/{id}.json')
@limiter.limit("20/second")
async def series_meta(request: Request, id: str):
    print(f"üì∫ Series meta request: {id}")
    
    if id.startswith('anime_') and anime_scrapers:
        parts = id.split('_')
        if len(parts) >= 3:
            source_site = parts[1]
            anime_title = ' '.join(parts[2:]).replace('_', ' ').title()
            
            meta = {
                'meta': {
                    'id': id,
                    'type': 'series',
                    'name': anime_title,
                    'poster': None,
                    'description': f"Anime disponibile su {source_site.upper()}",
                    'genres': ['Anime']
                }
            }
            return respond_with(meta)
    
    # Meta per serie normali
    meta = {
        'meta': {
            'id': id,
            'type': 'series',
            'name': f'Serie {id}',
            'poster': None,
            'description': f"Serie disponibile tramite provider configurati",
            'genres': ['Series']
        }
    }
    return respond_with(meta)

# STREAM ENDPOINT PRINCIPALE
@app.get('/{config:path}/stream/{type}/{id}.json')
@limiter.limit("5/second")
async def addon_stream(request: Request, config, type, id):
    if type not in MANIFEST['types']:
        raise HTTPException(status_code=404)
    
    streams = {'streams': []}
    print(f"üîç Stream request - Type: {type}, ID: {id}, Config: {config}")
    
    # GESTIONE ANIME PERSONALIZZATI
    if type == "series" and id.startswith('anime_') and anime_scrapers:
        print(f"üéå Custom anime stream request: {id}")
        
        try:
            parts = id.split('_')
            if len(parts) >= 3:
                source_site = parts[1]
                anime_title = ' '.join(parts[2:]).replace('_', ' ')
                
                if source_site in anime_scrapers:
                    scraper = anime_scrapers[source_site]
                    loop = asyncio.get_event_loop()
                    
                    # Cerca anime
                    search_results = await loop.run_in_executor(None, scraper.search, anime_title)
                    if search_results:
                        anime_url = search_results[0]['url']
                        
                        # Ottieni episodi
                        episodes = await loop.run_in_executor(None, scraper.get_episodes, anime_url)
                        if episodes:
                            first_episode = episodes[0]
                            stream_links = await loop.run_in_executor(None, scraper.get_stream_links, first_episode['url'])
                            
                            for stream in stream_links:
                                streams['streams'].append({
                                    'name': f"{Name} - {source_site.upper()}",
                                    'title': f'{Icon}{source_site.upper()} - {stream.get("quality", "HD")}',
                                    'url': stream['url'],
                                    'behaviorHints': {
                                        'notWebReady': stream.get('type') == 'iframe',
                                        'bingeGroup': f'anime_{source_site}'
                                    }
                                })
            
            if not streams['streams']:
                streams['streams'].append({
                    'title': f'{Icon}Anime non disponibile',
                    'url': 'https://example.com/not_found.mp4'
                })
                
        except Exception as e:
            print(f"‚ùå Error getting anime streams: {e}")
            
        return respond_with(streams)
    
    # GESTIONE FILM E SERIE NORMALI
    if (type == "movie" or type == "series") and (id.startswith('tt') or id.startswith('kitsu:')):
        if "|" in config:
            config_providers = config.split('|')
        elif "%7C" in config:
            config_providers = config.split('%7C')
        else:
            config_providers = []
            
        provider_maps = {name: "0" for name in provider_map.values()}
        for provider in config_providers:
            if provider in provider_map:
                provider_name = provider_map[provider]
                provider_maps[provider_name] = "1"
        
        MFP = "0"
        MFP_CREDENTIALS = None
        
        if "MFP[" in config:
            mfp_data = config.split("MFP[")[1].split(")")[0]
            MFP_url, MFP_password = mfp_data.split(",")
            MFP_password = MFP_password[:-2]
            MFP_CREDENTIALS = [MFP_url, MFP_password]
            if MFP_url and MFP_password:
                MFP = "1"
        
        async with AsyncSession(proxies=proxies) as client:
            # GESTIONE ANIME KITSU
            if id.startswith('kitsu:'):
                if provider_maps['ANIMEWORLD'] == "1" and AW == "1":
                    animeworld_urls = await animeworld(id, client)
                    if animeworld_urls:
                        print(f"AnimeWorld Found Results for {id}")
                        i = 0
                        for url in animeworld_urls:
                            if url:
                                title = "Original" if i == 0 else "Italian"
                                streams['streams'].append({
                                    'title': f'{Icon}AnimeWorld {title}', 
                                    'url': url
                                })
                                i += 1
            
            # GESTIONE FILM E SERIE NORMALI
            else:
                import time
                current_time = int(time.time())
                if 1743487223 <= current_time <= 1743544823:
                    streams['streams'].append({
                        'name': f"{Name} 4K",
                        'title': f'{Icon}Netflix/Prime Extractor 4K', 
                        'url': "https://cdn-cf-east.streamable.com/video/mp4/jkx9gr.mp4?Expires=1743457311748&Key-Pair-Id=APKAIEYUVEN4EVB2OKEQ&Signature=gpixXPFJb5huM8D6AMkbzNqmAON-9zBUVIN5AeWcHiXBVROSz6BlmctAVx0qpe-hM1DN3OO7YtIdBKKOk3IthF33agmVmVjSyNI-emjf~iuqxclbaousBJTPXMIjDQTxBxINr0SUbyS4MiIwhar~luiqqvbPHN9jS-AXT2r1chhZylE4Zol~bKSCCT10TzN3En630XMk0UiTFCgwoAxfitI4mnuCXu4M3-mcnN~kpxx9j6VgE0jVzBKFq9qYbi-CtWOCL7mVaVaCwrTPPe9syZVQgIlgQJt175raLM2G2~faR~wuDOda7KmGNJJH2hDfdd~-sPsr6SSNV0B9ZZ3eaw__"
                    })
                
                if MYSTERIUS == "1":
                    results = await cool(id, client)
                    if results:
                        print(f"Mysterius Found Results for {id}")
                        for resolution, link in results.items():
                            streams['streams'].append({
                                'title': f'{Icon}Mysterious {resolution}', 
                                'url': link, 
                                'behaviorHints': {'bingeGroup': f'mysterius{resolution}'}
                            })
                
                if provider_maps['STREAMINGCOMMUNITY'] == "1" and SC == "1":
                    SC_FAST_SEARCH = provider_maps.get('SC_FAST_SEARCH', '0')
                    url_streaming_community, quality_sc, slug_sc = await streaming_community(id, client, SC_FAST_SEARCH, MFP)
                    if url_streaming_community is not None:
                        print(f"StreamingCommunity Found Results for {id}")
                        if MFP == "1":
                            url_streaming_community = f'{MFP_url}/extractor/video?api_password={MFP_password}&d={url_streaming_community}&host=VixCloud&redirect_stream=false'
                            url_streaming_community = await transform_mfp(url_streaming_community, client)
                            if "hf.space" in MFP_url:
                                streams['streams'].append({
                                    "name": f'{Name}', 
                                    'title': f'{Icon}StreamingCommunity\n Sorry StreamingCommunity wont work, most likely, with MFP hosted on HuggingFace',
                                    'url': url_streaming_community
                                })
                            streams['streams'].append({
                                "name": f'{Name}\n{quality_sc} Max', 
                                'title': f'{Icon}StreamingCommunity\n {slug_sc.replace("-"," ").capitalize()}',
                                'url': url_streaming_community,
                                'behaviorHints': {'notWebReady': False, 'bingeGroup': f'streamingcommunity{quality_sc}'}
                            })
                        else:
                            streams['streams'].append({
                                "name": f'{Name}\n{quality_sc}p Max', 
                                'title': f'{Icon}StreamingCommunity\n {slug_sc.replace("-"," ").capitalize()}\n This will work only on a local instance',
                                'url': url_streaming_community,
                                'behaviorHints': {'proxyHeaders': {"request": {"user-agent": User_Agent}}, 'notWebReady': True, 'bingeGroup': f'streamingcommunity{quality_sc}'}
                            })
                
                # TUTTI GLI ALTRI PROVIDER ORIGINALI
                if provider_maps['LORDCHANNEL'] == "1" and LC == "1":
                    url_lordchannel, quality_lordchannel = await lordchannel(id, client)
                    if quality_lordchannel == "FULL HD" and url_lordchannel != None:
                        print(f"LordChannel Found Results for {id}")
                        streams['streams'].append({
                            'name': f"{Name}\n1080p",
                            'title': f'{Icon}LordChannel', 
                            'url': url_lordchannel,
                            'behaviorHints': {'bingeGroup': 'lordchannel1080'}
                        })
                    elif url_lordchannel != None:
                        print(f"LordChannel Found Results for {id}")
                        streams['streams'].append({
                            "name": f"{Name}\n720p",
                            'title': f'{Icon}LordChannel 720p', 
                            'url': url_lordchannel, 
                            'behaviorHints': {'bingeGroup': 'lordchannel720'}
                        })
                
                # ... AGGIUNGI TUTTI GLI ALTRI PROVIDER ORIGINALI QUI ...
        
        return respond_with(streams)
    
    # GESTIONE TV (CODICE ORIGINALE)
    elif type == "tv":
        # TUTTO IL CODICE TV ORIGINALE INVARIATO
        for channel in STREAM["channels"]:
            if channel["id"] == id:
                i = 0
                if 'url' in channel:
                    i = i+1
                    streams['streams'].append({
                        'title': f"{Icon}Server {i} " + f" "+ channel['name'] + " " + channel['title'] ,
                        'url': channel['url']
                    })
                # ... resto del codice TV originale ...
        
        if not streams['streams']:
            raise HTTPException(status_code=404)
        return respond_with(streams)
    
    if not streams['streams']:
        raise HTTPException(status_code=404)

    return respond_with(streams)

if __name__ == '__main__':
    import uvicorn
    print(f"üöÄ Starting MammaMiaAnime server on {HOST}:{PORT}")
    print(f"üéå Anime scrapers available: {ANIME_SCRAPERS_AVAILABLE}")
    print(f"üìã Manifest URL: http://{HOST}:{PORT}/manifest.json")
    uvicorn.run("run:app", host=HOST, port=PORT, log_level="info")
